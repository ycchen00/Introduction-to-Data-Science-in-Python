{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know the basics of what makes up a pandas dataframe, lets look at how we might actually clean\n",
    "some messy data. Now, there are many different approaches you can take to clean data, so this lecture is\n",
    "just one example of how you might tackle a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dfs=pd.read_html(\"https://en.wikipedia.org/wiki/College_admissions_in_the_United_States\")\n",
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>University</th>\n",
       "      <th>St</th>\n",
       "      <th>Ap-plied#</th>\n",
       "      <th>Over-allrate</th>\n",
       "      <th>Earlyrate</th>\n",
       "      <th>Regu-larrate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amherst</td>\n",
       "      <td>MA</td>\n",
       "      <td>5511</td>\n",
       "      <td>12%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Babson</td>\n",
       "      <td>MA</td>\n",
       "      <td>5511</td>\n",
       "      <td>29%</td>\n",
       "      <td>53%</td>\n",
       "      <td>21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Barnard</td>\n",
       "      <td>NY</td>\n",
       "      <td>5440</td>\n",
       "      <td>21%</td>\n",
       "      <td>45%</td>\n",
       "      <td>18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SUNY-Bing.</td>\n",
       "      <td>NY</td>\n",
       "      <td>28174</td>\n",
       "      <td>42%</td>\n",
       "      <td>56%</td>\n",
       "      <td>36%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Boston Coll</td>\n",
       "      <td>MA</td>\n",
       "      <td>34000</td>\n",
       "      <td>29%</td>\n",
       "      <td>40%</td>\n",
       "      <td>26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Boston U</td>\n",
       "      <td>MA</td>\n",
       "      <td>43979</td>\n",
       "      <td>45%</td>\n",
       "      <td>47%</td>\n",
       "      <td>44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bowdoin</td>\n",
       "      <td>ME</td>\n",
       "      <td>6716</td>\n",
       "      <td>16%</td>\n",
       "      <td>25%</td>\n",
       "      <td>14%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brown</td>\n",
       "      <td>RI</td>\n",
       "      <td>28742</td>\n",
       "      <td>10%</td>\n",
       "      <td>19%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Caltech[237]</td>\n",
       "      <td>CA</td>\n",
       "      <td>5225</td>\n",
       "      <td>17%</td>\n",
       "      <td>19%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Carleton</td>\n",
       "      <td>MN</td>\n",
       "      <td>5850</td>\n",
       "      <td>26%</td>\n",
       "      <td>41%</td>\n",
       "      <td>26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Carnegie-Mel</td>\n",
       "      <td>PA</td>\n",
       "      <td>17300</td>\n",
       "      <td>27%</td>\n",
       "      <td>26%</td>\n",
       "      <td>27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Claremont Mc</td>\n",
       "      <td>CA</td>\n",
       "      <td>5056</td>\n",
       "      <td>12%</td>\n",
       "      <td>29%</td>\n",
       "      <td>10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Colby</td>\n",
       "      <td>ME</td>\n",
       "      <td>5241</td>\n",
       "      <td>29%</td>\n",
       "      <td>50%</td>\n",
       "      <td>29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Colgate</td>\n",
       "      <td>NY</td>\n",
       "      <td>7795</td>\n",
       "      <td>29%</td>\n",
       "      <td>51%</td>\n",
       "      <td>27%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Columbia</td>\n",
       "      <td>NY</td>\n",
       "      <td>31851</td>\n",
       "      <td>7%</td>\n",
       "      <td>20%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Cooper Union</td>\n",
       "      <td>NY</td>\n",
       "      <td>3556</td>\n",
       "      <td>6%</td>\n",
       "      <td>9%</td>\n",
       "      <td>6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cornell</td>\n",
       "      <td>NY</td>\n",
       "      <td>37812</td>\n",
       "      <td>16%</td>\n",
       "      <td>33%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Dartmouth</td>\n",
       "      <td>NH</td>\n",
       "      <td>23110</td>\n",
       "      <td>9%</td>\n",
       "      <td>26%</td>\n",
       "      <td>8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Dickinson</td>\n",
       "      <td>PA</td>\n",
       "      <td>5843</td>\n",
       "      <td>40%</td>\n",
       "      <td>53%</td>\n",
       "      <td>28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Duke</td>\n",
       "      <td>NC</td>\n",
       "      <td>31600</td>\n",
       "      <td>12%</td>\n",
       "      <td>25%</td>\n",
       "      <td>11%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Elon</td>\n",
       "      <td>NC</td>\n",
       "      <td>10195</td>\n",
       "      <td>51%</td>\n",
       "      <td>86%</td>\n",
       "      <td>29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Emory</td>\n",
       "      <td>GA</td>\n",
       "      <td>17502</td>\n",
       "      <td>26%</td>\n",
       "      <td>38%</td>\n",
       "      <td>25%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>G Washington</td>\n",
       "      <td>DC</td>\n",
       "      <td>21759</td>\n",
       "      <td>33%</td>\n",
       "      <td>38%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Grinnell</td>\n",
       "      <td>IA</td>\n",
       "      <td>4554</td>\n",
       "      <td>30%</td>\n",
       "      <td>58%</td>\n",
       "      <td>28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Hamilton</td>\n",
       "      <td>NY</td>\n",
       "      <td>5107</td>\n",
       "      <td>27%</td>\n",
       "      <td>44%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Hanover</td>\n",
       "      <td>IN</td>\n",
       "      <td>3546</td>\n",
       "      <td>62%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Harvard[238]</td>\n",
       "      <td>MA</td>\n",
       "      <td>34302</td>\n",
       "      <td>6%</td>\n",
       "      <td>18%</td>\n",
       "      <td>4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Harvey Mudd</td>\n",
       "      <td>CA</td>\n",
       "      <td>3591</td>\n",
       "      <td>17%</td>\n",
       "      <td>20%</td>\n",
       "      <td>17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Johns Hopkins</td>\n",
       "      <td>MD</td>\n",
       "      <td>20496</td>\n",
       "      <td>18%</td>\n",
       "      <td>38%</td>\n",
       "      <td>16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Juilliard</td>\n",
       "      <td>NY</td>\n",
       "      <td>2319</td>\n",
       "      <td>7%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Texas A&amp;M</td>\n",
       "      <td>TX</td>\n",
       "      <td>31478</td>\n",
       "      <td>59%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Trinity</td>\n",
       "      <td>CT</td>\n",
       "      <td>7716</td>\n",
       "      <td>33%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>UC Berkeley</td>\n",
       "      <td>CA</td>\n",
       "      <td>61702</td>\n",
       "      <td>21%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>UC Davis</td>\n",
       "      <td>CA</td>\n",
       "      <td>49416</td>\n",
       "      <td>46%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>UC Irvine</td>\n",
       "      <td>CA</td>\n",
       "      <td>54532</td>\n",
       "      <td>36%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>UCLA</td>\n",
       "      <td>CA</td>\n",
       "      <td>72657</td>\n",
       "      <td>21%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>UC Merced</td>\n",
       "      <td>CA</td>\n",
       "      <td>13148</td>\n",
       "      <td>75%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>UC Riverside</td>\n",
       "      <td>CA</td>\n",
       "      <td>29888</td>\n",
       "      <td>61%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>UC San Diego</td>\n",
       "      <td>CA</td>\n",
       "      <td>60838</td>\n",
       "      <td>38%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>UC Santa Bar</td>\n",
       "      <td>CA</td>\n",
       "      <td>54831</td>\n",
       "      <td>42%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>UC Santa Cr.</td>\n",
       "      <td>CA</td>\n",
       "      <td>32954</td>\n",
       "      <td>60%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>U Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>25277</td>\n",
       "      <td>13%</td>\n",
       "      <td>18%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>U Delaware</td>\n",
       "      <td>DE</td>\n",
       "      <td>26534</td>\n",
       "      <td>53%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>U Florida</td>\n",
       "      <td>FL</td>\n",
       "      <td>29220</td>\n",
       "      <td>41%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>UNC Chapel H</td>\n",
       "      <td>NC</td>\n",
       "      <td>28491</td>\n",
       "      <td>27%</td>\n",
       "      <td>38%</td>\n",
       "      <td>16%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>U. Penn.</td>\n",
       "      <td>PA</td>\n",
       "      <td>31217</td>\n",
       "      <td>12%</td>\n",
       "      <td>25%</td>\n",
       "      <td>10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>U Puget Sou.</td>\n",
       "      <td>WA</td>\n",
       "      <td>6772</td>\n",
       "      <td>53%</td>\n",
       "      <td>88%</td>\n",
       "      <td>53%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>U Rochester[243]</td>\n",
       "      <td>NY</td>\n",
       "      <td>17428</td>\n",
       "      <td>36%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>USC</td>\n",
       "      <td>CA</td>\n",
       "      <td>46030</td>\n",
       "      <td>18%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>U. Virginia</td>\n",
       "      <td>VA</td>\n",
       "      <td>27200</td>\n",
       "      <td>29%</td>\n",
       "      <td>29%</td>\n",
       "      <td>23%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>U. Wisc-Mad.</td>\n",
       "      <td>WI</td>\n",
       "      <td>29008</td>\n",
       "      <td>54%</td>\n",
       "      <td>54%</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Vanderbilt</td>\n",
       "      <td>TN</td>\n",
       "      <td>28335</td>\n",
       "      <td>13%</td>\n",
       "      <td>25%</td>\n",
       "      <td>12%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Vassar</td>\n",
       "      <td>NY</td>\n",
       "      <td>7908</td>\n",
       "      <td>22%</td>\n",
       "      <td>43%</td>\n",
       "      <td>21%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Wake Forest</td>\n",
       "      <td>NC</td>\n",
       "      <td>11366</td>\n",
       "      <td>32%</td>\n",
       "      <td>43%</td>\n",
       "      <td>31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Wash &amp; Lee</td>\n",
       "      <td>VA</td>\n",
       "      <td>5970</td>\n",
       "      <td>18%</td>\n",
       "      <td>40%</td>\n",
       "      <td>15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Washington U</td>\n",
       "      <td>MO</td>\n",
       "      <td>28826</td>\n",
       "      <td>15%</td>\n",
       "      <td>31%</td>\n",
       "      <td>17%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Wesleyan</td>\n",
       "      <td>CT</td>\n",
       "      <td>10503</td>\n",
       "      <td>20%</td>\n",
       "      <td>45%</td>\n",
       "      <td>18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>William &amp; M.</td>\n",
       "      <td>VA</td>\n",
       "      <td>13651</td>\n",
       "      <td>31%</td>\n",
       "      <td>48%</td>\n",
       "      <td>29%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Williams</td>\n",
       "      <td>MA</td>\n",
       "      <td>7067</td>\n",
       "      <td>17%</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Yale</td>\n",
       "      <td>CT</td>\n",
       "      <td>28974</td>\n",
       "      <td>7%</td>\n",
       "      <td>16%</td>\n",
       "      <td>5%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          University  St  Ap-plied# Over-allrate Earlyrate Regu-larrate\n",
       "0            Amherst  MA       5511          12%       NaN          10%\n",
       "1             Babson  MA       5511          29%       53%          21%\n",
       "2            Barnard  NY       5440          21%       45%          18%\n",
       "3         SUNY-Bing.  NY      28174          42%       56%          36%\n",
       "4        Boston Coll  MA      34000          29%       40%          26%\n",
       "5           Boston U  MA      43979          45%       47%          44%\n",
       "6            Bowdoin  ME       6716          16%       25%          14%\n",
       "7              Brown  RI      28742          10%       19%          NaN\n",
       "8       Caltech[237]  CA       5225          17%       19%          NaN\n",
       "9           Carleton  MN       5850          26%       41%          26%\n",
       "10      Carnegie-Mel  PA      17300          27%       26%          27%\n",
       "11      Claremont Mc  CA       5056          12%       29%          10%\n",
       "12             Colby  ME       5241          29%       50%          29%\n",
       "13           Colgate  NY       7795          29%       51%          27%\n",
       "14          Columbia  NY      31851           7%       20%          NaN\n",
       "15      Cooper Union  NY       3556           6%        9%           6%\n",
       "16           Cornell  NY      37812          16%       33%          NaN\n",
       "17         Dartmouth  NH      23110           9%       26%           8%\n",
       "18         Dickinson  PA       5843          40%       53%          28%\n",
       "19              Duke  NC      31600          12%       25%          11%\n",
       "20              Elon  NC      10195          51%       86%          29%\n",
       "21             Emory  GA      17502          26%       38%          25%\n",
       "22      G Washington  DC      21759          33%       38%          NaN\n",
       "23          Grinnell  IA       4554          30%       58%          28%\n",
       "24          Hamilton  NY       5107          27%       44%          NaN\n",
       "25           Hanover  IN       3546          62%       NaN          62%\n",
       "26      Harvard[238]  MA      34302           6%       18%           4%\n",
       "27       Harvey Mudd  CA       3591          17%       20%          17%\n",
       "28     Johns Hopkins  MD      20496          18%       38%          16%\n",
       "29         Juilliard  NY       2319           7%       NaN           7%\n",
       "..               ...  ..        ...          ...       ...          ...\n",
       "51         Texas A&M  TX      31478          59%       NaN          59%\n",
       "52           Trinity  CT       7716          33%       NaN          NaN\n",
       "53       UC Berkeley  CA      61702          21%       NaN          NaN\n",
       "54          UC Davis  CA      49416          46%       NaN          NaN\n",
       "55         UC Irvine  CA      54532          36%       NaN          NaN\n",
       "56              UCLA  CA      72657          21%       NaN          NaN\n",
       "57         UC Merced  CA      13148          75%       NaN          NaN\n",
       "58      UC Riverside  CA      29888          61%       NaN          NaN\n",
       "59      UC San Diego  CA      60838          38%       NaN          NaN\n",
       "60      UC Santa Bar  CA      54831          42%       NaN          NaN\n",
       "61      UC Santa Cr.  CA      32954          60%       NaN          NaN\n",
       "62         U Chicago  IL      25277          13%       18%          NaN\n",
       "63        U Delaware  DE      26534          53%       NaN          53%\n",
       "64         U Florida  FL      29220          41%       NaN          41%\n",
       "65      UNC Chapel H  NC      28491          27%       38%          16%\n",
       "66          U. Penn.  PA      31217          12%       25%          10%\n",
       "67      U Puget Sou.  WA       6772          53%       88%          53%\n",
       "68  U Rochester[243]  NY      17428          36%       NaN          NaN\n",
       "69               USC  CA      46030          18%       NaN          18%\n",
       "70       U. Virginia  VA      27200          29%       29%          23%\n",
       "71      U. Wisc-Mad.  WI      29008          54%       54%          NaN\n",
       "72        Vanderbilt  TN      28335          13%       25%          12%\n",
       "73            Vassar  NY       7908          22%       43%          21%\n",
       "74       Wake Forest  NC      11366          32%       43%          31%\n",
       "75        Wash & Lee  VA       5970          18%       40%          15%\n",
       "76      Washington U  MO      28826          15%       31%          17%\n",
       "77          Wesleyan  CT      10503          20%       45%          18%\n",
       "78      William & M.  VA      13651          31%       48%          29%\n",
       "79          Williams  MA       7067          17%       NaN          NaN\n",
       "80              Yale  CT      28974           7%       16%           5%\n",
       "\n",
       "[81 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python programmers will often suggest that there many ways the language can be used to solve a particular problem. But that some are more appropriate than others. The best solutions are celebrated as Idiomatic Python and there are lots of great examples of this on StackOverflow and other websites.\n",
    "\n",
    "A sort of sub-language within Python, Pandas has its own set of idioms. We've alluded to some of these already, such as using vectorization whenever possible, and not using iterative loops if you don't need to. Several developers and users within the Panda's community have used the term pandorable for these idioms. I think it's a great term. So, I wanted to share with you a couple of key features of how you can make your code pandorable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "\n",
    "df = pd.read_csv('census.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first of these is called method chaining.\n",
    "# The general idea behind method chaining is that every method on an object \n",
    "# returns a reference to that object. The beauty of this is that you can \n",
    "# condense many different operations on a DataFrame, for instance, into one line \n",
    "# or at least one statement of code.\n",
    "# Here's an example of two pieces of code in pandas using our census data.\n",
    "\n",
    "# The first is the pandorable way to write the code with method chaining. In \n",
    "# this code, there's no in place flag being used and you can see that when we \n",
    "# first run a where query, then a dropna, then a set_index, and then a rename. \n",
    "# You might wonder why the whole statement is enclosed in parentheses and that's \n",
    "# just to make the statement more readable.\n",
    "(df.where(df['SUMLEV']==50)\n",
    "    .dropna()\n",
    "    .set_index(['STNAME','CTYNAME'])\n",
    "    .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The second example is a more traditional way of writing code.\n",
    "# There's nothing wrong with this code in the functional sense,\n",
    "# you might even be able to understand it better as a new person to the language.\n",
    "# It's just not as pandorable as the first example.\n",
    "\n",
    "df = df[df['SUMLEV']==50]\n",
    "df.set_index(['STNAME','CTYNAME'], inplace=True)\n",
    "df.rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, the key with any good idiom is to understand when it isn't helping you. \n",
    "# In this case, you can actually time both methods and see which one runs faster\n",
    "\n",
    "# We can put the approach into a function and pass the function into the timeit \n",
    "# function to count the time the parameter number allows us to choose how many \n",
    "# times we want to run the function. Here we will just set it to 1\n",
    "\n",
    "def first_approach():\n",
    "    global df\n",
    "    return (df.where(df['SUMLEV']==50)\n",
    "             .dropna()\n",
    "             .set_index(['STNAME','CTYNAME'])\n",
    "             .rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'}))\n",
    "    \n",
    "timeit.timeit(first_approach, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's test the second approach. As we notice, we use our global variable \n",
    "# df in the function. However, changing a global variable inside a function will \n",
    "# modify the variable even in a global scope and we do not want that to happen \n",
    "# in this case. Therefore, for selecting summary levels of 50 only, I create \n",
    "# a new dataframe for those records\n",
    "\n",
    "# Let's run this for once and see how fast it is\n",
    "\n",
    "def second_approach():\n",
    "    global df\n",
    "    new_df = df[df['SUMLEV']==50]\n",
    "    new_df.set_index(['STNAME','CTYNAME'], inplace=True)\n",
    "    return new_df.rename(columns={'ESTIMATESBASE2010': 'Estimates Base 2010'})\n",
    "timeit.timeit(second_approach, number=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As you can see, the second approach is much faster! \n",
    "# So, this is a particular example of a classic time readability trade off.\n",
    "\n",
    "# You'll see lots of examples on stock overflow and in documentation of people \n",
    "# using method chaining in their pandas. And so, I think being able to read and \n",
    "# understand the syntax is really worth your time. \n",
    "# Here's another pandas idiom. Python has a wonderful function called map, \n",
    "# which is sort of a basis for functional programming in the language. \n",
    "# When you want to use map in Python, you pass it some function you want called, \n",
    "# and some iterable, like a list, that you want the function to be applied to. \n",
    "# The results are that the function is called against each item in the list,\n",
    "# and there's a resulting list of all of the evaluations of that function.\n",
    "\n",
    "# Python has a similar function called applymap.\n",
    "# In applymap, you provide some function which should operate on each cell of a \n",
    "# DataFrame, and the return set is itself a DataFrame. Now I think applymap is \n",
    "# fine, but I actually rarely use it. Instead, I find myself often wanting to \n",
    "# map across all of the rows in a DataFrame. And pandas has a function that I \n",
    "# use heavily there, called apply. Let's look at an example.\n",
    "\n",
    "# Let's take our census DataFrame. \n",
    "# In this DataFrame, we have five columns for population estimates. \n",
    "# Each column corresponding with one year of estimates. It's quite reasonable to \n",
    "# want to create some new columns for \n",
    "# minimum or maximum values, and the apply function is an easy way to do this.\n",
    "\n",
    "\n",
    "# First, we need to write a function which takes in a particular row of data, \n",
    "# finds a minimum and maximum values, and returns a new row of data nd returns \n",
    "# a new row of data.  We'll call this function min_max, this is pretty straight \n",
    "# forward. We can create some small slice of a row by projecting the population \n",
    "# columns. Then use the NumPy min and max functions, and create a new series \n",
    "# with a label values represent the new values we want to apply.\n",
    "\n",
    "def min_max(row):\n",
    "    data = row[['POPESTIMATE2010',\n",
    "                'POPESTIMATE2011',\n",
    "                'POPESTIMATE2012',\n",
    "                'POPESTIMATE2013',\n",
    "                'POPESTIMATE2014',\n",
    "                'POPESTIMATE2015']]\n",
    "    return pd.Series({'min': np.min(data), 'max': np.max(data)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then we just need to call apply on the DataFrame. \n",
    "\n",
    "# Apply takes the function and the axis on which to operate as parameters. \n",
    "# Now, we have to be a bit careful, we've talked about axis zero being the rows \n",
    "# of the DataFrame in the past. But this parameter is really the parameter of \n",
    "# the index to use. So, to apply across all rows, which is applying on all \n",
    "# columns, you pass axis equal to one.\n",
    "df.apply(min_max, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Of course there's no need to limit yourself to returning a new series object. \n",
    "# If you're doing this as part of data cleaning your likely to find yourself \n",
    "# wanting to add new data to the existing DataFrame. In that case you just take \n",
    "# the row values and add in new columns indicating the max and minimum scores.\n",
    "# This is a regular part of my workflow when bringing in data and building \n",
    "# summary or descriptive statistics. \n",
    "# And is often used heavily with the merging of DataFrames.\n",
    "\n",
    "# Here we have a revised version of the function min_max\n",
    "# Instead of returning a separate series to display the min and max\n",
    "# We add two new columns in the original dataframe to store min and max\n",
    "\n",
    "def min_max(row):\n",
    "    data = row[['POPESTIMATE2010',\n",
    "                'POPESTIMATE2011',\n",
    "                'POPESTIMATE2012',\n",
    "                'POPESTIMATE2013',\n",
    "                'POPESTIMATE2014',\n",
    "                'POPESTIMATE2015']]\n",
    "    row['max'] = np.max(data)\n",
    "    row['min'] = np.min(data)\n",
    "    return row\n",
    "df.apply(min_max, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply is an extremely important tool in your toolkit. The reason I introduced \n",
    "# apply here is because you rarely see it used with large function definitions, \n",
    "# like we did. Instead, you typically see it used with lambdas. To get the most \n",
    "# of the discussions you'll see online, you're going to need to know how to \n",
    "# at least read lambdas. \n",
    "\n",
    "# Here's You can imagine how you might chain several apply calls with lambdas \n",
    "# together to create a readable yet succinct data manipulation script. One line \n",
    "# example of how you might calculate the max of the columns \n",
    "# using the apply function. \n",
    "rows = ['POPESTIMATE2010',\n",
    "        'POPESTIMATE2011',\n",
    "        'POPESTIMATE2012',\n",
    "        'POPESTIMATE2013',\n",
    "        'POPESTIMATE2014',\n",
    "        'POPESTIMATE2015']\n",
    "df.apply(lambda x: np.max(x[rows]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The beauty of the apply function is that it allows flexibility in doing \n",
    "# whatever manipulation that you desire, and the function you pass into apply \n",
    "# can be any customized function that you write. Let's say we want to divide the \n",
    "# states into four categories: Northeast, Midwest, South, and West\n",
    "# We can write a customized function that returns the region based on the state\n",
    "# the state regions information is obtained from Wikipedia\n",
    "\n",
    "def get_state_region(x):\n",
    "    northeast = ['Connecticut', 'Maine', 'Massachusetts', 'New Hampshire', \n",
    "                 'Rhode Island','Vermont','New York','New Jersey','Pennsylvania']\n",
    "    midwest = ['Illinois','Indiana','Michigan','Ohio','Wisconsin','Iowa',\n",
    "               'Kansas','Minnesota','Missouri','Nebraska','North Dakota',\n",
    "               'South Dakota']\n",
    "    south = ['Delaware','Florida','Georgia','Maryland','North Carolina',\n",
    "             'South Carolina','Virginia','District of Columbia','West Virginia',\n",
    "             'Alabama','Kentucky','Mississippi','Tennessee','Arkansas',\n",
    "             'Louisiana','Oklahoma','Texas']\n",
    "    west = ['Arizona','Colorado','Idaho','Montana','Nevada','New Mexico','Utah',\n",
    "            'Wyoming','Alaska','California','Hawaii','Oregon','Washington']\n",
    "    \n",
    "    if x in northeast:\n",
    "        return \"Northeast\"\n",
    "    elif x in midwest:\n",
    "        return \"Midwest\"\n",
    "    elif x in south:\n",
    "        return \"South\"\n",
    "    else:\n",
    "        return \"West\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we have the customized function, let's say we want to create a new column\n",
    "# called Region, which shows the state's region, we can use the customized \n",
    "# function and the apply function to do so. The customized function is supposed \n",
    "# to work on the state name column STNAME. So we will set the apply function on \n",
    "# the state name column and pass the customized function into the apply function\n",
    "df['state_region'] = df['STNAME'].apply(lambda x: get_state_region(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's see the results\n",
    "df[['STNAME','state_region']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are a couple of Pandas idioms. But I think there's many more, and I haven't talked about them here. So here's an unofficial assignment for you. Go look at some of the top ranked questions on pandas on Stack Overflow, and look at how some of the more experienced authors, answer those questions. Do you see any interesting patterns? Chime in on the course discussion forums and let's build some pandorable documents together.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
